{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1bqlR9Q3K0o",
        "outputId": "db2ae3e4-b3bc-4001-aa58-6799f59f7b21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[32m\\\u001b[0m \u001b[32m41.4 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m \u001b[33m0:00:03\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for flappy-bird-gymnasium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q https://github.com/PatrickKudo/flappy-bird-gymnasium/archive/refs/heads/main.zip\n",
        "!pip install -q optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7zI34373_Tr",
        "outputId": "28d140a7-21a8-4c45-ffc4-fcf9f4e79698"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing run_trial.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile run_trial.py\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pygame\n",
        "import gymnasium\n",
        "import flappy_bird_gymnasium\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "import argparse\n",
        "from statistics import mean, stdev\n",
        "\n",
        "\n",
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, input_channels, input_length, action_space):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "\n",
        "        # 1D Convolutional feature extractor\n",
        "        self.conv1d_features = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, 8, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(8, 16, kernel_size=2, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # Fully Connected feature extractor\n",
        "        self.dense_features = nn.Sequential(\n",
        "            nn.Linear(input_length * input_channels, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # Combined feature size\n",
        "        combined_feature_size = 144 + 128\n",
        "\n",
        "        # Value stream\n",
        "        self.value_stream = nn.Sequential(\n",
        "            nn.Linear(combined_feature_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        # Advantage stream\n",
        "        self.advantage_stream = nn.Sequential(\n",
        "            nn.Linear(combined_feature_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_space)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        conv_features = self.conv1d_features(state.unsqueeze(1))\n",
        "        dense_features = self.dense_features(state.view(state.size(0), -1))\n",
        "        combined_features = torch.cat((conv_features, dense_features), dim=1)\n",
        "\n",
        "        value = self.value_stream(combined_features)\n",
        "        advantages = self.advantage_stream(combined_features)\n",
        "        # from the paper: Q(s, a; θ, α, β) = V (s; θ, β) + (A(s, a; θ, α) − mean(A(s, a'; θ, α))).\n",
        "        qvals = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
        "        return qvals\n",
        "\n",
        "\n",
        "# Replay memory\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize misc. game settings\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "os.environ[\"SDL_AUDIODRIVER\"] = \"dummy\"\n",
        "pygame.init()\n",
        "\n",
        "# Establish Flappy Bird environment\n",
        "env = gymnasium.make(\n",
        "    \"FlappyBird-v0\", audio_on=False, render_mode=\"rgb_array\", use_lidar=False\n",
        ")\n",
        "\n",
        "# set up matplotlib display functionality\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# Check if GPU is available to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"cuda is available: \", torch.cuda.is_available())\n",
        "\n",
        "# Single transition of environment: map state-action pairs to rewards\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the parser\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# Adding arguments\n",
        "parser.add_argument(\"--batch_size\", type=int, help=\"The batch size for the model training\", required=True)\n",
        "parser.add_argument(\"--lr\", type=float, help=\"Learning rate\", required=True)\n",
        "parser.add_argument(\"--gamma\", type=float, help=\"Gamma value used in optimizations\", required=True)\n",
        "parser.add_argument(\"--tau\", type=float, help=\"Target network update rate\", required=True)\n",
        "\n",
        "# Parse the arguments\n",
        "args = parser.parse_args()\n",
        "\n",
        "BATCH_SIZE = args.batch_size\n",
        "GAMMA = args.gamma\n",
        "LR = args.lr\n",
        "TAU = args.tau\n",
        "\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 1000\n",
        "\n",
        "print(\"Batch size:\", BATCH_SIZE)\n",
        "print(\"Learning rate:\", LR)\n",
        "print(\"Gamma:\", GAMMA)\n",
        "print(\"Tau:\", TAU)\n",
        "\n",
        "# Get number of actions from gym action space\n",
        "n_actions = env.action_space.n\n",
        "# Get the number of state observations\n",
        "state, info = env.reset()\n",
        "n_observations = len(state)\n",
        "\n",
        "input_channels = 1\n",
        "input_length = 12\n",
        "policy_net = DuelingDQN(input_channels, input_length, n_actions).to(device)\n",
        "target_net = DuelingDQN(input_channels, input_length, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "# Set replay limit\n",
        "memory = ReplayMemory(2000)\n",
        "\n",
        "# Initialize step counter\n",
        "steps_done = 0\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state).max(1).indices.view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "# Determine episodes based on GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    num_episodes = 2000\n",
        "else:\n",
        "    num_episodes = 50\n",
        "\n",
        "# Start training loop\n",
        "for i_episode in range(num_episodes):\n",
        "    # Initialize the environment and get its state\n",
        "    state, info = env.reset()\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    for t in count():\n",
        "        action = select_action(state)\n",
        "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if terminated:\n",
        "            next_state = None\n",
        "        else:\n",
        "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the policy network)\n",
        "        optimize_model()\n",
        "\n",
        "        # Soft update of the target network's weights\n",
        "        # θ′ ← τ θ + (1 −τ )θ′\n",
        "        target_net_state_dict = target_net.state_dict()\n",
        "        policy_net_state_dict = policy_net.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "        target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            break\n",
        "\n",
        "print(\"std: \", stdev(episode_durations))\n",
        "print(\"mean: \", mean(episode_durations))\n",
        "print(\"maxL: \", max(episode_durations))\n",
        "\n",
        "with open('dqcnn_results.txt', 'w') as f:\n",
        "    f.write(f\"{mean(episode_durations)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5bFg_pOodUc"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import gc\n",
        "from statistics import mean, stdev\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    # Define Hyperparameters\n",
        "    lr = trial.suggest_float('lr', 1e-5, 1e-3, log=True)\n",
        "    BATCH_SIZE = trial.suggest_categorical('batch_size', [64, 128, 256])\n",
        "    GAMMA = trial.suggest_float('gamma', 0.88, 0.99)\n",
        "    TAU = trial.suggest_float('tau', 0.001, 0.01)\n",
        "\n",
        "    # Run python script with parameters\n",
        "    !python run_trial.py --lr $lr --batch_size $BATCH_SIZE --gamma $GAMMA --tau $TAU\n",
        "    gc.collect()\n",
        "\n",
        "    with open('dqcnn_results.txt', 'r') as f:\n",
        "        mean_reward = float(f.read())\n",
        "    print(f\"Mean reward: {mean_reward}\")\n",
        "    return mean_reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaU8mHwvodRD",
        "outputId": "7093cc86-cc82-47e8-a8ec-77f0cab7c5aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-24 02:59:44,160] A new study created in memory with name: no-name-65d6cdfe-58cc-45db-95a1-21559fd0c74b\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "cuda is available:  True\n",
            "Batch size: 256\n",
            "Learning rate: 1.2054082240560545e-05\n",
            "Gamma: 0.9664969978823046\n",
            "Tau: 0.007326869282989046\n",
            "std:  14.025398377580043\n",
            "mean:  58.3675\n",
            "maxL:  122\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-24 03:16:17,531] Trial 0 finished with value: 58.3675 and parameters: {'lr': 1.2054082240560545e-05, 'batch_size': 256, 'gamma': 0.9664969978823046, 'tau': 0.007326869282989046}. Best is trial 0 with value: 58.3675.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward: 58.3675\n",
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "cuda is available:  True\n",
            "Batch size: 128\n",
            "Learning rate: 0.00019908739372021862\n",
            "Gamma: 0.9854873185924804\n",
            "Tau: 0.003087789523396862\n",
            "std:  42.3303753123811\n",
            "mean:  81.2525\n",
            "maxL:  350\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-24 03:35:22,866] Trial 1 finished with value: 81.2525 and parameters: {'lr': 0.00019908739372021862, 'batch_size': 128, 'gamma': 0.9854873185924804, 'tau': 0.003087789523396862}. Best is trial 1 with value: 81.2525.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward: 81.2525\n",
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "cuda is available:  True\n",
            "Batch size: 256\n",
            "Learning rate: 0.00010207719094496091\n",
            "Gamma: 0.9133164525292045\n",
            "Tau: 0.0031290697553260152\n",
            "std:  38.883551857469584\n",
            "mean:  76.3645\n",
            "maxL:  312\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-24 03:56:36,269] Trial 2 finished with value: 76.3645 and parameters: {'lr': 0.00010207719094496091, 'batch_size': 256, 'gamma': 0.9133164525292045, 'tau': 0.0031290697553260152}. Best is trial 1 with value: 81.2525.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward: 76.3645\n",
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "cuda is available:  True\n",
            "Batch size: 64\n",
            "Learning rate: 0.0009811163667618748\n",
            "Gamma: 0.9585385406069935\n",
            "Tau: 0.007998939936210122\n",
            "std:  42.91147859544837\n",
            "mean:  82.5985\n",
            "maxL:  391\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-24 04:13:43,705] Trial 3 finished with value: 82.5985 and parameters: {'lr': 0.0009811163667618748, 'batch_size': 64, 'gamma': 0.9585385406069935, 'tau': 0.007998939936210122}. Best is trial 3 with value: 82.5985.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward: 82.5985\n",
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "cuda is available:  True\n",
            "Batch size: 64\n",
            "Learning rate: 4.343443847000902e-05\n",
            "Gamma: 0.900754602159937\n",
            "Tau: 0.009253021977158622\n",
            "std:  26.326240772784352\n",
            "mean:  69.1615\n",
            "maxL:  216\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-24 04:27:59,927] Trial 4 finished with value: 69.1615 and parameters: {'lr': 4.343443847000902e-05, 'batch_size': 64, 'gamma': 0.900754602159937, 'tau': 0.009253021977158622}. Best is trial 3 with value: 82.5985.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward: 69.1615\n",
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "cuda is available:  True\n",
            "Batch size: 256\n",
            "Learning rate: 0.00015187711625010592\n",
            "Gamma: 0.893116210055214\n",
            "Tau: 0.007987394057849175\n",
            "std:  31.98478559025568\n",
            "mean:  73.002\n",
            "maxL:  235\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-24 04:48:53,574] Trial 5 finished with value: 73.002 and parameters: {'lr': 0.00015187711625010592, 'batch_size': 256, 'gamma': 0.893116210055214, 'tau': 0.007987394057849175}. Best is trial 3 with value: 82.5985.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward: 73.002\n",
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "cuda is available:  True\n",
            "Batch size: 64\n",
            "Learning rate: 0.0007767394880339421\n",
            "Gamma: 0.8914419299993633\n",
            "Tau: 0.004163987622645837\n",
            "std:  32.03157795040695\n",
            "mean:  74.255\n",
            "maxL:  348\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-24 05:04:46,201] Trial 6 finished with value: 74.255 and parameters: {'lr': 0.0007767394880339421, 'batch_size': 64, 'gamma': 0.8914419299993633, 'tau': 0.004163987622645837}. Best is trial 3 with value: 82.5985.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward: 74.255\n",
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "cuda is available:  True\n",
            "Batch size: 128\n",
            "Learning rate: 0.0005632499188606747\n",
            "Gamma: 0.8837067783013438\n",
            "Tau: 0.002346034173940196\n",
            "std:  13.98044155679861\n",
            "mean:  54.4555\n",
            "maxL:  202\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-24 05:17:31,845] Trial 7 finished with value: 54.4555 and parameters: {'lr': 0.0005632499188606747, 'batch_size': 128, 'gamma': 0.8837067783013438, 'tau': 0.002346034173940196}. Best is trial 3 with value: 82.5985.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward: 54.4555\n",
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "cuda is available:  True\n",
            "Batch size: 64\n",
            "Learning rate: 0.0001488330027418392\n",
            "Gamma: 0.9229620679133506\n",
            "Tau: 0.006140939340181215\n",
            "std:  38.94709627050458\n",
            "mean:  79.6055\n",
            "maxL:  348\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-24 05:33:47,734] Trial 8 finished with value: 79.6055 and parameters: {'lr': 0.0001488330027418392, 'batch_size': 64, 'gamma': 0.9229620679133506, 'tau': 0.006140939340181215}. Best is trial 3 with value: 82.5985.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward: 79.6055\n",
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "cuda is available:  True\n",
            "Batch size: 128\n",
            "Learning rate: 1.2139341639058215e-05\n",
            "Gamma: 0.9767927980452539\n",
            "Tau: 0.004191595375387928\n",
            "std:  14.667376383149868\n",
            "mean:  56.194\n",
            "maxL:  199\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-24 05:46:50,945] Trial 9 finished with value: 56.194 and parameters: {'lr': 1.2139341639058215e-05, 'batch_size': 128, 'gamma': 0.9767927980452539, 'tau': 0.004191595375387928}. Best is trial 3 with value: 82.5985.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward: 56.194\n",
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "cuda is available:  True\n",
            "Batch size: 64\n",
            "Learning rate: 0.0004056184246135034\n",
            "Gamma: 0.949541160481272\n",
            "Tau: 0.009253431457415097\n",
            "std:  35.938634694877685\n",
            "mean:  73.927\n",
            "maxL:  404\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-24 06:02:06,643] Trial 10 finished with value: 73.927 and parameters: {'lr': 0.0004056184246135034, 'batch_size': 64, 'gamma': 0.949541160481272, 'tau': 0.009253431457415097}. Best is trial 3 with value: 82.5985.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward: 73.927\n",
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "cuda is available:  True\n",
            "Batch size: 128\n",
            "Learning rate: 0.000258004402277914\n",
            "Gamma: 0.9887658357052606\n",
            "Tau: 0.0013554696839748678\n",
            "std:  38.223798939962805\n",
            "mean:  73.082\n",
            "maxL:  407\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-04-24 06:19:15,019] Trial 11 finished with value: 73.082 and parameters: {'lr': 0.000258004402277914, 'batch_size': 128, 'gamma': 0.9887658357052606, 'tau': 0.0013554696839748678}. Best is trial 3 with value: 82.5985.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward: 73.082\n",
            "pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
            "cuda is available:  True\n",
            "Batch size: 128\n",
            "Learning rate: 4.26817143166533e-05\n",
            "Gamma: 0.9512479275085138\n",
            "Tau: 0.005420801907344046\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(\"  Value: \", trial.value)\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "# save tuna results\n",
        "study.trials_dataframe().to_csv('dqcnn_tuna_results.csv')\n",
        "!cp dqcnn_tuna_results.csv \"/content/drive/MyDrive/MSAI/spr24/RL/\"\n",
        "# visualize\n",
        "optuna.visualization.plot_optimization_history(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdT8etbS4fif"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}